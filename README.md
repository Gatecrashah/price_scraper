# Multi-Site Price Monitoring System

A sophisticated, automated price monitoring system that tracks product prices across **Bj√∂rn Borg** (Finnish site) and **Fitnesstukku** (Finnish fitness supplements), with intelligent GitHub issue-based product management and robust structured data extraction.

## üèóÔ∏è Architecture Overview

This system uses a modern, resilient architecture with structured data prioritization:

### Core Components

```
‚îú‚îÄ‚îÄ scrapers/                    # Modular scraper architecture
‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py         # Common interface & utilities
‚îÇ   ‚îú‚îÄ‚îÄ bjornborg.py            # JSON-LD structured data scraper
‚îÇ   ‚îî‚îÄ‚îÄ fitnesstukku.py         # dataTrackingView structured data scraper
‚îú‚îÄ‚îÄ price_monitor.py            # Main orchestration & GitHub integration
‚îú‚îÄ‚îÄ email_sender.py             # Resend API email notifications
‚îú‚îÄ‚îÄ email_templates.py          # HTML email generation
‚îú‚îÄ‚îÄ product_manager.py          # GitHub issue comment processing
‚îú‚îÄ‚îÄ products.yaml               # Product configuration (auto-managed)
‚îî‚îÄ‚îÄ price_history.json          # Historical price data (auto-managed)
```

### Key Features

- **ü§ñ Automated GitHub Issues**: Newly discovered products create issues for track/ignore decisions
- **üìä Structured Data Extraction**: Prioritizes JSON-LD and dataLayer over brittle CSS
- **üìß Smart Email Alerts**: Multi-site HTML notifications via Resend API
- **üìà Price Analytics**: Trend analysis and monthly reports
- **üîÑ Zero-Maintenance**: Fully automated via GitHub Actions

## üöÄ How It Works

### 1. **Daily Monitoring Cycle** (9:00 AM UTC)
```mermaid
graph LR
    A[GitHub Actions] --> B[Scrape Products]
    B --> C[Detect Changes]
    C --> D[Update History]
    D --> E[Send Email Alerts]
    E --> F[Discover New Products]
    F --> G[Create GitHub Issues]
```

### 2. **Product Discovery & Management**
When new products are discovered:
1. **GitHub Issue Created** with product details
2. **Email Notification** sent to you
3. **Reply "track" or "ignore"** via email or GitHub
4. **Automated Processing** updates configuration
5. **Issue Auto-Closed** with confirmation

### 3. **Structured Data Extraction**

**Bj√∂rn Borg** (JSON-LD Schema.org):
```json
{
  "@type": "Product",
  "name": "Essential Socks 10-pack",
  "sku": "10004564_MP001",
  "offers": {
    "price": 35.96,
    "priceCurrency": "EUR",
    "availability": "InStock"
  }
}
```

**Fitnesstukku** (dataTrackingView ecommerce):
```javascript
{
  "event": "productDetail",
  "ecommerce": {
    "detail": {
      "products": [{
        "id": "590-1",
        "name": "Whey-80, 4 kg, Double Rich Chocolate",
        "brand": "Star Nutrition",
        "price": "86.90"
      }]
    }
  }
}
```

## üìã Current Products Tracked

### Bj√∂rn Borg (9 products)
- **Essential Socks 10-pack**: 4 variants (Multi, Variant 1, 2, + 1 new)
- **Centre Crew Sweatshirt**: 3 colors (Grey, Blue, Black)
- **Essential Ankle Socks**: 2 variants (3-pack, 10-pack)

### Fitnesstukku (2 products)
- **Whey-80 Protein 4kg** (Star Nutrition)
- **Creatine Monohydrate 500g** (Star Nutrition)

## ‚öôÔ∏è Setup & Configuration

### Prerequisites
- Python 3.11+
- UV package manager
- Resend API key
- GitHub repository with Actions enabled

### Environment Variables
Create `.env` file:
```bash
RESEND_API_KEY="your-resend-api-key"
EMAIL_TO="your-email@example.com"
```

### Installation
```bash
# Install dependencies
uv sync

# Test scrapers
uv run python3 -c "from scrapers import BjornBorgScraper, FitnesstukuScraper; print('‚úÖ Scrapers ready')"

# Run full monitoring cycle
source .env && uv run python3 price_monitor.py
```

## üì¶ Adding New Products

### Method 1: Automatic Discovery (Recommended)

The system automatically discovers new Bj√∂rn Borg Essential 10-pack variants. When found:

1. **GitHub Issue Created** with product details
2. **Email Sent** to your configured address
3. **Reply Process**:
   - Reply to email with `track` or `ignore`
   - Or comment on GitHub issue directly
4. **Automatic Processing** updates `products.yaml`
5. **Issue Closed** with confirmation

### Method 2: Manual Addition

#### Adding Bj√∂rn Borg Products

1. **Find the Product URL**
   ```bash
   # Example: https://www.bjornborg.com/fi/essential-socks-10-pack-12345-mp001/
   ```

2. **Add to products.yaml**
   ```yaml
   products:
     bjornborg:
       - name: "Essential Socks 10-pack (New Variant)"
         url: "/fi/essential-socks-10-pack-12345-mp001/"
         product_id: "12345"
         site: "bjornborg"
         category: "socks"  # or "apparel"
         status: "track"
   ```

3. **Commit Changes**
   ```bash
   git add products.yaml
   git commit -m "Add new Bj√∂rn Borg product: Essential Socks 10-pack (New Variant)"
   git push
   ```

#### Adding Fitnesstukku Products

1. **Find the Product URL**
   ```bash
   # Example: https://www.fitnesstukku.fi/casein-protein-2kg/ABC123.html
   ```

2. **Add to products.yaml**
   ```yaml
   products:
     fitnesstukku:
       - name: "Casein Protein 2kg"
         url: "https://www.fitnesstukku.fi/casein-protein-2kg/ABC123.html"
         product_id: "ABC123"
         site: "fitnesstukku"
         category: "protein"  # or "supplements", "nutrition"
         status: "track"
   ```

3. **Test the Product**
   ```bash
   source .env && uv run python3 -c "
   from scrapers import FitnesstukuScraper
   scraper = FitnesstukuScraper()
   product = scraper.scrape_product_page('https://www.fitnesstukku.fi/casein-protein-2kg/ABC123.html')
   print('‚úÖ Product data:', product)
   "
   ```

4. **Commit Changes**
   ```bash
   git add products.yaml
   git commit -m "Add new Fitnesstukku product: Casein Protein 2kg"
   git push
   ```

### Method 3: Temporary Testing

For testing without permanent addition:

```bash
source .env && uv run python3 -c "
from scrapers import BjornBorgScraper
scraper = BjornBorgScraper()

# Test a Bj√∂rn Borg product
product = scraper.scrape_product_page('/fi/new-product-url/')
print('Product data:', product)
"
```

## üéØ Product Configuration Reference

### Required Fields
```yaml
- name: "Product Display Name"          # Human-readable name
  url: "/relative/or/full/url/"         # Product page URL
  product_id: "12345"                   # Unique identifier (extracted from URL/page)
  site: "bjornborg"                     # Site identifier: "bjornborg" or "fitnesstukku"
  category: "socks"                     # Product category for organization
  status: "track"                       # Tracking status: "track" or "ignore"
```

### Category Guidelines

**Bj√∂rn Borg:**
- `socks` - All sock products (Essential, ankle, etc.)
- `apparel` - Clothing items (Centre Crew, sweaters, etc.)

**Fitnesstukku:**
- `protein` - Whey, casein, protein powders
- `supplements` - Creatine, vitamins, etc.
- `nutrition` - General nutrition products

### Status Options
- `track` - Monitor price changes and send alerts
- `ignore` - Keep in config but don't monitor (useful for discontinued products)

## üìä GitHub Actions Workflows

### Daily Price Monitor
- **Schedule**: 9:00 AM UTC daily
- **Triggers**: Automatic + manual via GitHub UI
- **Actions**: Scrape ‚Üí Detect changes ‚Üí Send emails ‚Üí Discover products

### Comment Processor
- **Trigger**: GitHub issue comments
- **Function**: Process "track"/"ignore" commands
- **Security**: Only repository owner can manage products

## üõ†Ô∏è Development & Testing

### Local Testing
```bash
# Test individual scrapers
source .env && uv run python3 -c "from scrapers import BjornBorgScraper; print(BjornBorgScraper().scrape_all_products())"

# Test full monitoring
source .env && uv run python3 price_monitor.py

# Test email system
source .env && uv run python3 email_sender.py
```

### Adding New Sites

To add support for a new e-commerce site:

1. **Create New Scraper**
   ```python
   # scrapers/newsite.py
   from .base_scraper import BaseScraper
   
   class NewSiteScraper(BaseScraper):
       def extract_structured_data(self, soup, url):
           # Implement structured data extraction
           pass
       
       def extract_fallback_data(self, soup, url):
           # Implement CSS selector fallback
           pass
   ```

2. **Update Package**
   ```python
   # scrapers/__init__.py
   from .newsite import NewSiteScraper
   __all__ = ['BjornBorgScraper', 'FitnesstukuScraper', 'NewSiteScraper']
   ```

3. **Integrate with Monitor**
   ```python
   # price_monitor.py
   self.newsite_scraper = NewSiteScraper()
   ```

4. **Add Products Configuration**
   ```yaml
   # products.yaml
   products:
     newsite:
       - name: "Product Name"
         url: "https://newsite.com/product"
         # ... other fields
   ```

## üîí Security & Best Practices

### Environment Security
- **Never commit API keys** - Use `.env` file (gitignored)
- **Use GitHub Secrets** for production API keys
- **Validate input** in product_manager.py

### Scraping Ethics
- **Respectful delays** - 1 second between requests
- **User-Agent headers** - Identify as legitimate browser
- **Error handling** - Fail gracefully, don't overwhelm servers

### Data Management
- **price_history.json** - Managed by GitHub Actions only
- **products.yaml** - Can be edited manually or via issues
- **Backup important data** - History file contains valuable price data

## üìà Analytics & Reporting

The system includes advanced analytics:

- **Monthly Reports** - Comprehensive price trend analysis
- **Seasonal Patterns** - Holiday and seasonal price tracking  
- **Best Deal Alerts** - Automated notifications for significant drops
- **Historical Tracking** - Long-term price history (365 days retention)

## üêõ Troubleshooting

### Common Issues

**"No products found"**
```bash
# Check product URLs are accessible
curl -I "https://www.bjornborg.com/fi/your-product-url/"

# Verify products.yaml syntax
uv run python3 -c "import yaml; print(yaml.safe_load(open('products.yaml')))"
```

**"Email not sending"**
```bash
# Test email configuration
source .env && uv run python3 email_sender.py
```

**"Scraper failing"**
```bash
# Test individual product
source .env && uv run python3 -c "
from scrapers import BjornBorgScraper
scraper = BjornBorgScraper()
product = scraper.scrape_product_page('/fi/your-product-url/')
print(product)
"
```

### Debug Mode
```bash
# Enable debug logging
source .env && PYTHONPATH=. uv run python3 -c "
import logging
logging.basicConfig(level=logging.DEBUG)
from price_monitor import PriceMonitor
monitor = PriceMonitor()
# Your debug code here
"
```

## üìÑ License & Contributing

This is a personal price monitoring system. Feel free to fork and adapt for your own use cases.

### Recent Improvements
- ‚úÖ **Modular scraper architecture** with structured data prioritization
- ‚úÖ **GitHub issue-based product management** replacing email workflows
- ‚úÖ **100% structured data extraction** for maximum reliability
- ‚úÖ **Enhanced robustness** against website changes
- ‚úÖ **Comprehensive error handling** with loud failures

---

**ü§ñ Automated by GitHub Actions | üìß Powered by Resend API | üèóÔ∏è Built with Python & UV**